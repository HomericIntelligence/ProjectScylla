% BibTeX References for ProjectScylla Research
% Generated: 2026-02-07
% Contains only cited references from paper.tex

% =============================================================================
% Self-Reference: Dryrun Paper
% =============================================================================

@misc{scylla2026dryrun,
  title={Taming {Scylla}: Understanding the multi-headed agentic daemon of the coding seas},
  author={Villmow, Micah},
  year={2026},
  howpublished={ProjectScylla Technical Report},
  note={Dryrun evaluation of the Scylla framework using Claude Sonnet 4.5 on a Hello World task. Available at: \url{https://github.com/HomericIntelligence/ProjectScylla/tree/main/docs/arxiv/dryrun}}
}

% =============================================================================
% Core Benchmark and Framework References
% =============================================================================

@article{liu2023agentbench,
  title={{AgentBench}: Evaluating {LLMs} as Agents},
  author={Liu, Xiao and others},
  journal={arXiv preprint arXiv:2308.03688},
  year={2024},
  url={https://arxiv.org/abs/2308.03688},
  note={Multi-turn agent evaluation across operating systems, databases, and knowledge graphs}
}

@article{jimenez2024swebench,
  title={{SWE-bench}: Can Language Models Resolve Real-world Github Issues?},
  author={Jimenez, Carlos E. and others},
  journal={arXiv preprint arXiv:2310.06770},
  year={2024},
  url={https://arxiv.org/abs/2310.06770},
  note={Benchmark for evaluating LLMs on real GitHub issues}
}

@article{yao2024taubench,
  title={{TAU-bench}: A Benchmark for Tool-Augmented {LLMs}},
  author={Yao, Shunyu and others},
  journal={arXiv preprint arXiv:2406.12045},
  year={2024},
  url={https://arxiv.org/abs/2406.12045},
  note={Benchmark for evaluating agents' ability to use external tools}
}

@article{zhu2024promptbench,
  title={{PromptBench}: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts},
  author={Zhu, Kaijie and others},
  journal={arXiv preprint arXiv:2306.04528},
  year={2024},
  url={https://arxiv.org/abs/2306.04528},
  note={Benchmark for prompt evaluation and robustness}
}

@article{polo2024efficient,
  title={Efficient multi-prompt evaluation of LLMs},
  author={Polo, Felipe Maia and others},
  journal={arXiv preprint arXiv:2405.17202},
  year={2024},
  url={https://arxiv.org/abs/2405.17202},
  note={Framework for efficient LLM evaluation}
}

@misc{projectodyssey,
  title={{ProjectOdyssey}: Comprehensive Agent Orchestration Framework},
  author={{Homeric Intelligence}},
  year={2025},
  howpublished={\url{https://github.com/HomericIntelligence/Projectodyssey}},
  note={Accessed: 2025-12-30. Git hash: 011a3ff}
}

@misc{anthropic2024claude,
  title={{Claude Code}: Agentic {CLI} Tool for Software Development},
  author={{Anthropic}},
  year={2024},
  howpublished={\url{https://www.anthropic.com/claude/code}},
  note={AI-powered command-line interface for coding tasks}
}

@misc{gao2024lmevalharness,
  title={{lm-evaluation-harness}: A Framework for Few-Shot Language Model Evaluation},
  author={Gao, Leo and others},
  year={2024},
  howpublished={\url{https://github.com/EleutherAI/lm-evaluation-harness}},
  note={General-purpose LLM evaluation framework}
}

@misc{safetynet,
  title={safety-net: {Claude Code} Plugin for Dangerous Operation Blocking},
  author={{CC-Marketplace}},
  howpublished={\url{https://github.com/cc-marketplace/safety-net}},
  year={2025},
  note={Security plugin for Claude Code CLI}
}

@misc{ccmarketplace,
  author={Anand Tyagi},
  title={{CC-Marketplace}: Community Marketplace for {Claude Code} Plugins and Skills},
  howpublished={\url{https://github.com/cc-marketplace}},
  year={2025},
  note={Community-driven plugin repository}
}
