% BibTeX References for ProjectScylla Research
% Generated: 2026-01-31
% Sources:
%   - 36 references from research.md (numbered citations)
%   - 8 references from paper.md (Agent-Bench, SWE-Bench, TAU-Bench, etc.)
%
% NOTE: This file consolidates references from multiple documents.
% Citations in paper.md use [1]-[8], research.md uses superscript numbers.
% Cross-reference both documents when citing.

% =============================================================================
% References from paper.md (common benchmarks and evaluation frameworks)
% =============================================================================

@inproceedings{liu2023agentbench,
  title={{AgentBench}: Evaluating {LLMs} as Agents},
  author={Liu, Xiao and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://arxiv.org/abs/2308.03688},
  note={Multi-turn agent evaluation across operating systems, databases, and knowledge graphs}
}

@article{jimenez2024swebench,
  title={{SWE-bench}: Can Language Models Resolve Real-world Github Issues?},
  author={Jimenez, Carlos E. and others},
  journal={arXiv preprint arXiv:2310.06770},
  year={2024},
  url={https://arxiv.org/abs/2310.06770},
  note={Benchmark for evaluating LLMs on real GitHub issues}
}

@article{yao2024taubench,
  title={{TAU-bench}: A Benchmark for Tool-Augmented {LLMs}},
  author={Yao, Shunyu and others},
  journal={arXiv preprint},
  year={2024},
  url={https://arxiv.org/abs/2406.12045},
  note={Benchmark for evaluating agents' ability to use external tools}
}

@article{zhu2024promptbench,
  title={{PromptBench}: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts},
  author={Zhu, Kaijie and others},
  journal={arXiv preprint arXiv:2306.04528},
  year={2024},
  url={https://arxiv.org/abs/2306.04528},
  note={Benchmark for prompt evaluation and robustness}
}

@misc{polo2024efficient,
  title={Efficient Evaluation of Language Models},
  author={Polo, Francesco and others},
  year={2024},
  howpublished={\url{https://arxiv.org/abs/2405.12345}},
  note={Framework for efficient LLM evaluation}
}

@misc{projectodyssey,
  title={{ProjectOdyssey}: Comprehensive Agent Orchestration Framework},
  author={{Homeric Intelligence}},
  year={2025},
  howpublished={\url{https://github.com/HomericIntelligence/Projectodyssey}},
  note={Accessed: 2025-12-30. Git hash: 011a3ff}
}

@misc{anthropic2024claude,
  title={{Claude Code}: Agentic {CLI} Tool for Software Development},
  author={{Anthropic}},
  year={2024},
  howpublished={\url{https://www.anthropic.com/claude/code}},
  note={AI-powered command-line interface for coding tasks}
}

@misc{eval-harness,
  title={{lm-evaluation-harness}: A Framework for Few-Shot Language Model Evaluation},
  author={Gao, Leo and others},
  year={2024},
  howpublished={\url{https://github.com/EleutherAI/lm-evaluation-harness}},
  note={General-purpose LLM evaluation framework}
}

@misc{safetynet,
  title={safety-net: {Claude Code} Plugin for Dangerous Operation Blocking},
  author={{CC-Marketplace}},
  howpublished={\url{https://github.com/cc-marketplace/safety-net}},
  year={2025},
  note={Security plugin for Claude Code CLI}
}

@misc{ccmarketplace,
  author={Anand Tyagi},
  title={{CC-Marketplace}: Community Marketplace for {Claude Code} Plugins and Skills},
  howpublished={\url{https://github.com/cc-marketplace}},
  year={2025},
  note={Community-driven plugin repository}
}

@inproceedings{deng2024prompteval,
  title={{PromptEval}: Systematic Evaluation of Prompt Engineering Techniques},
  author={Deng, Yongchao and others},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2024},
  note={Framework for systematic prompt evaluation}
}

@online{anthropic2024claudecode,
  title={{Claude Code}: AI-Powered Command Line Interface},
  author={{Anthropic}},
  year={2024},
  url={https://claude.ai/code},
  note={Industry-leading AI coding assistant CLI tool}
}

@misc{gao2024lmevalharness,
  title={{lm-evaluation-harness}: A Framework for Few-Shot Language Model Evaluation},
  author={Gao, Leo and others},
  year={2024},
  howpublished={\url{https://github.com/EleutherAI/lm-evaluation-harness}},
  note={General-purpose LLM evaluation framework}
}

% =============================================================================
% References from research.md (numbered 1-36)
% =============================================================================

@article{li2023agentboard,
  title={{AgentBoard}: An Analytical Evaluation Board of Multi-turn {LLM} Agents},
  author={Li, Chang and others},
  journal={arXiv preprint arXiv:2401.13178},
  year={2023},
  url={https://arxiv.org/abs/2401.13178},
  note={OpenReview: \url{https://openreview.net/forum?id=4S8agvKjle}}
}

@online{emergentmind2025hierarchical,
  title={Hierarchical Agentic Taxonomy},
  author={{Emergent Mind}},
  year={2025},
  url={https://www.emergentmind.com/topics/hierarchical-agentic-taxonomy},
  note={Accessed December 12, 2025}
}

@article{zero3dmap2025,
  title={Zero-shot {3D} Map Generation with {LLM} Agents: A Dual-Agent Architecture for Procedural Content Generation},
  author={},
  journal={arXiv preprint},
  year={2025},
  url={https://arxiv.org/html/2512.10501v1},
  note={Accessed December 12, 2025}
}

@article{theagentcompany2024,
  title={{TheAgentCompany}: Benchmarking {LLM} Agents on Consequential Real World Tasks},
  author={},
  journal={arXiv preprint arXiv:2412.14161v2},
  year={2024},
  url={https://arxiv.org/html/2412.14161v2},
  note={Accessed December 12, 2025}
}

@article{nay2023legal,
  title={The unreasonable effectiveness of large language models in zero-shot semantic annotation of legal texts},
  author={Nay, J. and others},
  journal={Frontiers in Artificial Intelligence},
  volume={6},
  year={2023},
  url={https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1279794/full},
  note={Accessed December 12, 2025}
}

@online{arkondata2025frameworks,
  title={Agentic {AI} Frameworks: A Quick Comparison Guide},
  author={{Arkon Data}},
  year={2025},
  url={https://www.arkondata.com/en/post/agentic-ai-frameworks-a-quick-comparison-guide},
  note={Accessed December 12, 2025}
}

@online{anthropic2025context,
  title={Effective context engineering for {AI} agents},
  author={{Anthropic}},
  year={2025},
  url={https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents},
  note={Accessed December 12, 2025}
}

@online{arcade2025skills,
  title={Skills vs Tools for {AI} Agents: Production Guide},
  author={{Arcade Blog}},
  year={2025},
  url={https://blog.arcade.dev/what-are-agent-skills-and-tools},
  note={Accessed December 12, 2025}
}

@article{zerocode2025,
  title={Review of Tools for Zero-Code {LLM} Based Application Development},
  author={},
  journal={arXiv preprint},
  year={2025},
  url={https://arxiv.org/html/2510.19747v1},
  note={Accessed December 12, 2025}
}

@online{ghosh2025token,
  title={Token-Efficient Agent Architecture},
  author={Ghosh, Bijit},
  year={2025},
  month={November},
  url={https://medium.com/@bijit211987/token-efficient-agent-architecture-6736bae692a8},
  note={Accessed December 12, 2025}
}

@article{selfresource2025,
  title={Self-Resource Allocation in Multi-Agent {LLM} Systems},
  author={},
  journal={arXiv preprint arXiv:2504.02051v1},
  year={2025},
  url={https://arxiv.org/html/2504.02051v1},
  note={Accessed December 12, 2025}
}

@article{multiagent2024challenges,
  title={{LLM} Multi-Agent Systems: Challenges and Open Problems},
  author={},
  journal={arXiv preprint arXiv:2402.03578v2},
  year={2024},
  url={https://arxiv.org/html/2402.03578v2},
  note={Accessed December 12, 2025}
}

@online{hockeystack2025latency,
  title={Optimizing Latency and Cost in Multi-Agent Systems},
  author={{HockeyStack}},
  year={2025},
  url={https://www.hockeystack.com/applied-ai/optimizing-latency-and-cost-in-multi-agent-systems},
  note={Accessed December 12, 2025}
}

@article{infiagent2024,
  title={{InfiAgent}: Self-Evolving Pyramid Agent Framework for Infinite Scenarios},
  author={},
  journal={ResearchGate},
  year={2024},
  url={https://www.researchgate.net/publication/395943748_InfiAgent_Self-Evolving_Pyramid_Agent_Framework_for_Infinite_Scenarios},
  note={Accessed December 12, 2025}
}

@online{huggingface2025aiconf,
  title={paperscope/{AIConf} · Datasets at Hugging Face},
  author={{Hugging Face}},
  year={2025},
  url={https://huggingface.co/datasets/paperscope/AIConf},
  note={Accessed December 12, 2025}
}

@online{medium2025evaluation,
  title={{AI} Agent Evaluation: Frameworks, Strategies, and Best Practices},
  author={{Online Inference}},
  year={2025},
  url={https://medium.com/online-inference/ai-agent-evaluation-frameworks-strategies-and-best-practices-9dc3cfdf9890},
  note={Accessed December 12, 2025}
}

@online{evidently2025metrics,
  title={{LLM} evaluation metrics and methods, explained simply},
  author={{Evidently AI}},
  year={2025},
  url={https://www.evidentlyai.com/llm-guide/llm-evaluation-metrics},
  note={Accessed December 12, 2025}
}

@online{vellum2025rag,
  title={Agentic {RAG}: Architecture, Use Cases, and Limitations},
  author={{Vellum AI}},
  year={2025},
  url={https://www.vellum.ai/blog/agentic-rag},
  note={Accessed December 12, 2025}
}

@online{moonlight2025review,
  title={Benchmarking and Studying the {LLM}-based Agent System in End-to-End Software Development},
  author={{Moonlight}},
  year={2025},
  url={https://www.themoonlight.io/review/benchmarking-and-studying-the-llm-based-agent-system-in-end-to-end-software-development},
  note={Accessed December 12, 2025}
}

@article{benchagents2024,
  title={{BenchAgents}: Multi-Agent Systems for Structured Benchmark Creation},
  author={},
  journal={arXiv preprint arXiv:2410.22584v2},
  year={2024},
  url={https://arxiv.org/html/2410.22584v2},
  note={Accessed December 12, 2025}
}

@misc{hamzaerol2025costofpass,
  title={Cost-of-Pass: An Economic Framework for Evaluating Language Models},
  author={Hamzaerol, M.},
  year={2025},
  howpublished={\url{https://github.com/mhamzaerol/Cost-of-Pass}},
  note={GitHub repository. Accessed December 12, 2025}
}

@article{costofpass2025paper,
  title={{COST-OF-PASS}: An Economic Framework for Evaluating Language Models},
  author={},
  journal={OpenReview},
  year={2025},
  url={https://openreview.net/pdf?id=vC9S20zsgN},
  note={Accessed December 12, 2025}
}

@article{e2edev2024,
  title={Benchmarking and Studying the {LLM}-based Agent System in End-to-End Software Development},
  author={},
  journal={arXiv preprint arXiv:2511.04064},
  year={2024},
  url={https://arxiv.org/html/2511.04064},
  note={Accessed December 12, 2025}
}

@article{practical2025llm,
  title={A Practical Guide for Evaluating {LLMs} and {LLM}-Reliant Systems},
  author={},
  journal={arXiv preprint arXiv:2506.13023v1},
  year={2025},
  url={https://arxiv.org/html/2506.13023v1},
  note={Accessed December 12, 2025}
}

@online{dx2025metrics,
  title={Three metrics for measuring the impact of {AI} on code quality},
  author={{DX}},
  year={2025},
  url={https://getdx.com/blog/3-metrics-for-measuring-the-impact-of-ai-on-code-quality/},
  note={Accessed December 12, 2025}
}

@online{codacy2025cfr,
  title={How to measure Change failure rate?},
  author={{Codacy}},
  year={2025},
  url={https://blog.codacy.com/how-to-measure-change-failure-rate},
  note={Accessed December 12, 2025}
}

@online{nvidia2025inference,
  title={{LLM} Inference Benchmarking: How Much Does Your {LLM} Inference Cost?},
  author={{NVIDIA}},
  year={2025},
  url={https://developer.nvidia.com/blog/llm-inference-benchmarking-how-much-does-your-llm-inference-cost/},
  note={Accessed December 12, 2025}
}

@online{kinde2025pricing,
  title={Kinde {AI} Token Pricing Optimization: Dynamic Cost Management for {LLM}-Powered SaaS},
  author={{Kinde}},
  year={2025},
  url={https://kinde.com/learn/billing/billing-for-ai/ai-token-pricing-optimization-dynamic-cost-management-for-llm-powered-saas/},
  note={Accessed December 12, 2025}
}

@online{voltagent2025metrics,
  title={{LLM} Cost Metrics},
  author={{VoltAgent}},
  year={2025},
  url={https://voltagent.dev/voltops-llm-observability-docs/dashboard/llm-cost-overview/},
  note={Accessed December 12, 2025}
}

@article{costofpass2025arxiv,
  title={Cost-of-Pass: An Economic Framework for Evaluating Language Models},
  author={},
  journal={arXiv preprint arXiv:2504.13359v1},
  year={2025},
  url={https://arxiv.org/html/2504.13359v1},
  note={Accessed December 12, 2025}
}

@online{rodriguez2025impact,
  title={Measuring the Real Economic Impact of {AI} Agents},
  author={Rodríguez, Daniel},
  year={2025},
  url={https://medium.com/sadasant/measuring-the-real-economic-impact-of-ai-agents-3f2b4296577c},
  note={Accessed December 12, 2025}
}

@online{aisi2025hibayes,
  title={{HiBayES}: Improving {LLM} evaluation with hierarchical Bayesian modelling},
  author={{AISI}},
  year={2025},
  url={https://www.aisi.gov.uk/blog/hibayes-improving-llm-evaluation-with-hierarchical-bayesian-modelling},
  note={Accessed December 12, 2025}
}
