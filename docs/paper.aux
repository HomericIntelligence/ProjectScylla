\relax
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{liu2023agentbench}
\citation{jimenez2024swebench}
\citation{yao2024taubench}
\citation{zhu2024promptbench}
\citation{polo2024efficient}
\citation{anthropic2024claude}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{jimenez2024swebench}
\citation{liu2023agentbench}
\citation{yao2024taubench}
\citation{zhu2024promptbench}
\citation{polo2024efficient}
\citation{gao2024lmevalharness}
\citation{projectodyssey}
\citation{safetynet}
\citation{ccmarketplace}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{3}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Test Methodology}{4}{section.3}\protected@file@percent }
\newlabel{sec:methodology}{{3}{4}{Test Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Experimental Design}{4}{subsection.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces LLM-as-Judge Evaluation Categories}}{4}{table.caption.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Deduction Calibration Scale (for subjective assessment)}}{5}{table.caption.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Grade Scale}}{5}{table.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Test-001: Hello World Baseline}{6}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Rubric Categories and Weights}}{7}{table.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Tiered Ablation Strategy}{7}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{sec:tiered-ablation}{{3.1.2}{7}{Tiered Ablation Strategy}{subsubsection.3.1.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Testing Tiers (Ablation Study Framework)}}{7}{table.caption.5}\protected@file@percent }
\citation{projectodyssey}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Dimensional Search Space}{8}{subsection.3.2}\protected@file@percent }
\newlabel{sec:dimensions}{{3.2}{8}{Dimensional Search Space}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Agent Complexity Axis (Tiers 0-6)}{8}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Agent Complexity Axis}}{8}{table.caption.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Prompt Complexity Axis}{9}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Prompt Complexity Axis}}{9}{table.caption.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Skill Complexity Axis}{9}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Skill Complexity Axis}}{9}{table.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Agent Hierarchy Axis}{9}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Agent Hierarchy Axis}}{10}{table.caption.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Test Metrics}{10}{section.4}\protected@file@percent }
\newlabel{sec:metrics}{{4}{10}{Test Metrics}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Performance Metrics}{10}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Quality Metrics}{11}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Efficiency and Cost Metrics}{11}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Model Pricing (as of January 2026)}}{12}{table.caption.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Test Configuration}{12}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Hardware and Infrastructure}{12}{subsection.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Hardware and Infrastructure}}{12}{table.caption.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Software Stack}{12}{subsection.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Software Stack}}{13}{table.caption.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Model Configuration}{13}{subsection.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Execution Models (performing the tasks)}}{13}{table.caption.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Test Cases}{14}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Pull Request (PR) Selection Criteria}{14}{subsection.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Size Categories}}{14}{table.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Workflow Categories}{14}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Test Case Matrix}{14}{subsection.6.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces Workflow Categories}}{15}{table.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Results}{15}{section.7}\protected@file@percent }
\newlabel{sec:results}{{7}{15}{Results}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Pipeline Validation (Dryrun Overview)}{15}{subsection.7.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {16}{\ignorespaces Tier Summary (Dryrun)}}{16}{table.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Cost-of-Pass Analysis}{16}{subsection.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Cost-of-Pass by tier. Grouped bar chart showing CoP (cost per successful run) across T0--T6 with logarithmic scale. T5 achieves the Frontier CoP at \$0.065, while T6 is highest at \$0.247, demonstrating that architectural complexity amplifies costs without quality improvement on ceiling-constrained tasks.}}{17}{figure.caption.17}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fig06_cop_by_tier}{{1}{17}{Cost-of-Pass by tier. Grouped bar chart showing CoP (cost per successful run) across T0--T6 with logarithmic scale. T5 achieves the Frontier CoP at \$0.065, while T6 is highest at \$0.247, demonstrating that architectural complexity amplifies costs without quality improvement on ceiling-constrained tasks}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Token Analysis}{18}{subsection.7.3}\protected@file@percent }
\newlabel{sec:token-analysis}{{7.3}{18}{Token Analysis}{subsection.7.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {17}{\ignorespaces Token Breakdown}}{18}{table.caption.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Latency Analysis}{18}{subsection.7.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {18}{\ignorespaces Latency Breakdown}}{18}{table.caption.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Token distribution by tier and type. Stacked bar chart showing the breakdown of input, output, cache create, and cache read tokens across T0--T6. Cache read tokens dominate (79--95\%), consistent with prompt caching efficacy. However, T6's 218K cache reads versus T0's 113K illustrate the Token Efficiency Chasm, where architectural enhancements double token consumption without quality gains.}}{19}{figure.caption.18}\protected@file@percent }
\newlabel{fig:fig07_token_distribution}{{2}{19}{Token distribution by tier and type. Stacked bar chart showing the breakdown of input, output, cache create, and cache read tokens across T0--T6. Cache read tokens dominate (79--95\%), consistent with prompt caching efficacy. However, T6's 218K cache reads versus T0's 113K illustrate the Token Efficiency Chasm, where architectural enhancements double token consumption without quality gains}{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Latency breakdown by tier. Stacked bar chart showing agent execution time versus judge evaluation time across T0--T6. Judge evaluation dominates (77--86\% of total latency), ranging from 128--178 seconds, because three independent judges evaluate each run. This overhead is task-specific; on complex multi-step tasks, agent time would dominate instead.}}{20}{figure.caption.20}\protected@file@percent }
\newlabel{fig:fig13_latency}{{3}{20}{Latency breakdown by tier. Stacked bar chart showing agent execution time versus judge evaluation time across T0--T6. Judge evaluation dominates (77--86\% of total latency), ranging from 128--178 seconds, because three independent judges evaluate each run. This overhead is task-specific; on complex multi-step tasks, agent time would dominate instead}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Judge Agreement}{21}{subsection.7.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Per-judge scoring variance across tiers. Box plots showing score distributions for each judge model (Opus 4.5, Sonnet 4.5, Haiku 4.5) faceted by tier. Opus exhibits the tightest distribution (most conservative), Haiku the widest (most generous), revealing systematic inter-judge bias that affects aggregate score reliability.}}{21}{figure.caption.22}\protected@file@percent }
\newlabel{fig:fig02_judge_variance}{{4}{21}{Per-judge scoring variance across tiers. Box plots showing score distributions for each judge model (Opus 4.5, Sonnet 4.5, Haiku 4.5) faceted by tier. Opus exhibits the tightest distribution (most conservative), Haiku the widest (most generous), revealing systematic inter-judge bias that affects aggregate score reliability}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Inter-judge agreement scatter matrix. Pairwise scatter plots showing score correlations between all judge pairs (Opus-Sonnet, Opus-Haiku, Sonnet-Haiku) with Spearman and Pearson correlation coefficients. Low-to-moderate correlations reveal systematic bias between judges rather than strong agreement, with Opus-Sonnet showing the highest concordance (Pearson r=0.706).}}{22}{figure.caption.23}\protected@file@percent }
\newlabel{fig:fig14_judge_agreement}{{5}{22}{Inter-judge agreement scatter matrix. Pairwise scatter plots showing score correlations between all judge pairs (Opus-Sonnet, Opus-Haiku, Sonnet-Haiku) with Spearman and Pearson correlation coefficients. Low-to-moderate correlations reveal systematic bias between judges rather than strong agreement, with Opus-Sonnet showing the highest concordance (Pearson r=0.706)}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Criteria Breakdown}{23}{subsection.7.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {19}{\ignorespaces Per-Criteria Performance Comparison}}{23}{table.caption.24}\protected@file@percent }
\newlabel{tab:tab04_criteria_performance}{{19}{23}{Per-Criteria Performance Comparison}{table.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Per-criteria scores by tier. Grouped bar chart showing mean scores for five weighted categories (functional correctness, code quality, proportionality, build pipeline, overall quality) across T0--T6. Perfect functional scores (1.00) contrast with variance in proportionality (0.90--1.00) and overall quality (0.93--1.00).}}{23}{figure.caption.25}\protected@file@percent }
\newlabel{fig:fig09_criteria_by_tier}{{6}{23}{Per-criteria scores by tier. Grouped bar chart showing mean scores for five weighted categories (functional correctness, code quality, proportionality, build pipeline, overall quality) across T0--T6. Perfect functional scores (1.00) contrast with variance in proportionality (0.90--1.00) and overall quality (0.93--1.00)}{figure.caption.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Discussion}{24}{section.8}\protected@file@percent }
\newlabel{sec:discussion}{{8}{24}{Discussion}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}What the Dryrun Tells Us}{24}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Cost-Performance Trade-offs}{24}{subsection.8.2}\protected@file@percent }
\newlabel{sec:cost-tradeoffs}{{8.2}{24}{Cost-Performance Trade-offs}{subsection.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Judge Behavior}{25}{subsection.8.3}\protected@file@percent }
\newlabel{sec:judge-behavior}{{8.3}{25}{Judge Behavior}{subsection.8.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Limitations}{25}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusions}{26}{section.9}\protected@file@percent }
\bibstyle{plain}
\bibdata{references}
\bibcite{ccmarketplace}{1}
\@writefile{toc}{\contentsline {section}{\numberline {10}Further Work}{27}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Acknowledgements}{27}{section.11}\protected@file@percent }
\bibcite{anthropic2024claude}{2}
\bibcite{safetynet}{3}
\bibcite{gao2024lmevalharness}{4}
\bibcite{projectodyssey}{5}
\bibcite{jimenez2024swebench}{6}
\bibcite{liu2023agentbench}{7}
\bibcite{polo2024efficient}{8}
\bibcite{yao2024taubench}{9}
\bibcite{zhu2024promptbench}{10}
\@writefile{toc}{\contentsline {section}{\numberline {A}Detailed Metric Definitions}{28}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.0.1}Process Metrics}{28}{subsubsection.A.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.0.2}Statistical Reporting}{29}{subsubsection.A.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Data Dictionary and Generated Outputs}{29}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Reproducibility Checklist}{29}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}Additional Figures}{31}{appendix.D}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Variance Analysis}{31}{subsection.D.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Cost Analysis}{31}{subsection.D.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}Judge Analysis}{31}{subsection.D.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.4}Diagnostic Metrics}{31}{subsection.D.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Score distribution across tiers (T0--T6). Box plots with jittered points show the distributional shape of composite scores. The tight clustering between 0.94--0.98 across all tiers is consistent with the ceiling effect on this trivially simple task, as discussed in Section~\ref {sec:discussion}.}}{32}{figure.caption.27}\protected@file@percent }
\newlabel{fig:fig01_score_variance_by_tier}{{7}{32}{Score distribution across tiers (T0--T6). Box plots with jittered points show the distributional shape of composite scores. The tight clustering between 0.94--0.98 across all tiers is consistent with the ceiling effect on this trivially simple task, as discussed in Section~\ref {sec:discussion}}{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Grade distribution heatmap by tier. Heatmap showing the proportion of each grade (A+ through F) per tier, with count annotations. The dominance of A+ grades across all tiers visually illustrates the ceiling effect, where architectural enhancements cannot improve already-perfect task performance.}}{33}{figure.caption.28}\protected@file@percent }
\newlabel{fig:fig05_grade_heatmap}{{8}{33}{Grade distribution heatmap by tier. Heatmap showing the proportion of each grade (A+ through F) per tier, with count annotations. The dominance of A+ grades across all tiers visually illustrates the ceiling effect, where architectural enhancements cannot improve already-perfect task performance}{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Cost-quality Pareto frontier. Scatter plot of mean cost per run versus mean score for each tier, with Pareto-efficient points highlighted. Points along the frontier represent architecturally optimal trade-offs, revealing which configurations deliver maximum quality at minimum cost, as analyzed in Section~\ref {sec:cost-tradeoffs}.}}{34}{figure.caption.29}\protected@file@percent }
\newlabel{fig:fig08_cost_quality_pareto}{{9}{34}{Cost-quality Pareto frontier. Scatter plot of mean cost per run versus mean score for each tier, with Pareto-efficient points highlighted. Points along the frontier represent architecturally optimal trade-offs, revealing which configurations deliver maximum quality at minimum cost, as analyzed in Section~\ref {sec:cost-tradeoffs}}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Judge score variance overall. Box plot and standard deviation bars showing scoring distributions for each judge model (Opus 4.5, Sonnet 4.5, Haiku 4.5) across all evaluations. Systematic differences in judge harshness reveal evaluation bias, with implications for multi-judge aggregation strategies discussed in Section~\ref {sec:judge-behavior}.}}{35}{figure.caption.30}\protected@file@percent }
\newlabel{fig:fig17_judge_variance_overall}{{10}{35}{Judge score variance overall. Box plot and standard deviation bars showing scoring distributions for each judge model (Opus 4.5, Sonnet 4.5, Haiku 4.5) across all evaluations. Systematic differences in judge harshness reveal evaluation bias, with implications for multi-judge aggregation strategies discussed in Section~\ref {sec:judge-behavior}}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Metric correlation heatmap (Spearman $\rho $). Heatmap showing pairwise correlations between quality metrics (score, pass-rate) and cost metrics (cost, tokens, duration). Weak correlations between cost and quality suggest that increased spending does not guarantee better outcomes on ceiling-constrained tasks, supporting the economic analysis in Section~\ref {sec:cost-tradeoffs}.}}{35}{figure.caption.31}\protected@file@percent }
\newlabel{fig:fig20_metric_correlation_heatmap}{{11}{35}{Metric correlation heatmap (Spearman $\rho $). Heatmap showing pairwise correlations between quality metrics (score, pass-rate) and cost metrics (cost, tokens, duration). Weak correlations between cost and quality suggest that increased spending does not guarantee better outcomes on ceiling-constrained tasks, supporting the economic analysis in Section~\ref {sec:cost-tradeoffs}}{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Cost versus quality regression at subtest level. Scatter plot of mean cost versus mean score per subtest, with OLS regression line and 95\% confidence band. The near-flat or negative slope suggests that higher costs do not predict better quality, reinforcing the cost-quality decoupling phenomenon discussed in Section~\ref {sec:cost-tradeoffs}.}}{36}{figure.caption.32}\protected@file@percent }
\newlabel{fig:fig21_cost_quality_regression}{{12}{36}{Cost versus quality regression at subtest level. Scatter plot of mean cost versus mean score per subtest, with OLS regression line and 95\% confidence band. The near-flat or negative slope suggests that higher costs do not predict better quality, reinforcing the cost-quality decoupling phenomenon discussed in Section~\ref {sec:cost-tradeoffs}}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Implementation Rate versus Pass-Rate. Scatter plot showing the relationship between binary Pass-Rate (y-axis) and continuous Implementation Rate (x-axis) across all runs, colored by tier.}}{37}{figure.caption.33}\protected@file@percent }
\newlabel{fig:fig26_impl_rate_vs_pass_rate}{{13}{37}{Implementation Rate versus Pass-Rate. Scatter plot showing the relationship between binary Pass-Rate (y-axis) and continuous Implementation Rate (x-axis) across all runs, colored by tier}{figure.caption.33}{}}
\gdef \@abspage@last{37}
