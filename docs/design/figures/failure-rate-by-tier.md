# Figure 03: Failure Rate by Tier

> **Type**: Variance Analysis
> **Source**: `scylla/analysis/figures/variance.py:55-119`
> **Generated by**: `fig03_failure_rate_by_tier()`

## Overview

Figure 03 visualizes the **distribution of letter grades (S/A/B/C/D/F)** across testing tiers and agent models as a normalized stacked bar chart. The figure displays grade proportions rather than raw counts, enabling direct comparison of grade distributions across tiers with different sample sizes.

Unlike traditional "failure rate" metrics that show binary pass/fail, this figure provides **granular grade-level insights** showing how agent performance quality distributes across the full grading spectrum. This reveals not just whether agents fail, but how they fail (D vs F) and succeed (S vs A vs B).

The figure is faceted by `agent_model`, allowing comparison of grade distributions across different AI models (e.g., Claude Opus, Claude Sonnet) at each tier.

## Purpose

This figure addresses three critical evaluation questions:

1. **Grade Distribution Patterns**: How do grades distribute across the spectrum (S/A/B/C/D/F) at each tier?
2. **Tier Progression**: Do higher tiers (with more capabilities) show better grade distributions?
3. **Model Comparison**: How do different agent models compare in their grade distributions at the same tier?

**Key Use Cases**:

- Identify tiers where failures concentrate (high F/D proportions)
- Detect grade distribution anomalies (e.g., bimodal distributions)
- Compare model robustness across tiers
- Validate expected tier progression (T0 → T6 should show improvement)
- Guide tier optimization by identifying problem areas

## Data Source

**Input DataFrame**: `runs_df` (standardized runs DataFrame from analysis pipeline)

**Required Columns**:

- `agent_model` (str): Agent model identifier (e.g., "claude-opus-4", "claude-sonnet-3-5")
- `tier` (str): Testing tier (e.g., "T0", "T1", ..., "T6")
- `grade` (str): Letter grade assigned to run (S/A/B/C/D/F)

**Data Grouping**: Runs are grouped by `["agent_model", "tier", "grade"]` and counted.

**Proportion Calculation**:

```python
# Count runs per (agent_model, tier, grade) combination
grade_counts = runs_df.groupby(["agent_model", "tier", "grade"]).size()

# Calculate total runs per (agent_model, tier)
total_per_tier = grade_counts.groupby(["agent_model", "tier"]).sum()

# Compute proportions (normalized stacking)
proportion = grade_counts / total_per_tier
```

**Sample Size Annotation**: Each tier label includes the sample size (e.g., "T0 (n=240)") to provide context for interpretation.

## Mathematical Formulas

### Grade Proportion

The **proportion** of runs receiving grade $g$ in tier $t$ for model $m$ is:

$$
P(g, t, m) = \frac{N(g, t, m)}{\sum_{g' \in \text{Grades}} N(g', t, m)}
$$

Where:

- $P(g, t, m)$ = Proportion of runs with grade $g$ in tier $t$ for model $m$
- $N(g, t, m)$ = Count of runs with grade $g$ in tier $t$ for model $m$
- $\text{Grades} = \{S, A, B, C, D, F\}$

**Normalization Property**:
$$
\sum_{g \in \text{Grades}} P(g, t, m) = 1.0
$$

### Failure Rate (Derived Metric)

While not explicitly shown in the figure, the **failure rate** can be derived by summing failing grade proportions:

$$
\text{Failure Rate}(t, m) = P(D, t, m) + P(F, t, m)
$$

Where:

- $P(D, t, m)$ = Proportion of D grades (marginal)
- $P(F, t, m)$ = Proportion of F grades (failing)

**Pass Rate** (complement):
$$
\text{Pass Rate}(t, m) = P(S, t, m) + P(A, t, m) + P(B, t, m) + P(C, t, m)
$$

Assuming the default pass threshold of 0.60 (grade B), otherwise adjust to include only passing grades.

### Grade Distribution Statistics

**Median Grade**: The grade $g$ where cumulative proportion crosses 0.5:

$$
\text{Median Grade}(t, m) = \arg\min_g \left( \sum_{g' \leq g} P(g', t, m) \geq 0.5 \right)
$$

**Modal Grade**: The grade with the highest proportion:

$$
\text{Modal Grade}(t, m) = \arg\max_g P(g, t, m)
$$

## Theoretical Foundation

### Expected Tier Progression

**Hypothesis**: As tiers increase (T0 → T6), agents gain more capabilities, which should improve grade distributions.

**Expected Pattern**:

- **T0 (Prompts Only)**: High failure rate (F/D), low success rate (S/A/B)
- **T1-T3 (Skills/Tools/Delegation)**: Gradual improvement, shifting distribution toward passing grades
- **T4-T5 (Hierarchy/Hybrid)**: Significant improvement, majority of grades in B/A/S range
- **T6 (Super)**: Best grade distribution, minimal F/D, high S/A concentration

**Counter-Indicators** (anomalies to investigate):

- **Tier regression**: Higher tier shows worse distribution than lower tier
- **Bimodal distribution**: Sharp peaks at both high and low grades (indicates inconsistent performance)
- **Plateau effect**: No improvement beyond certain tier (diminishing returns)

### Grade Distribution Interpretation

**Healthy Distribution** (well-calibrated rubrics):

- Continuous distribution across grades (not clustered at extremes)
- Modal grade around B/A (pass threshold)
- Small but non-zero S grade proportion (achievable excellence)
- Small F proportion (clear failures are rare)

**Problematic Distribution**:

- **Bimodal** (F/S peaks): Rubric may have binary criteria rather than gradable spectrum
- **Uniform** (flat across all grades): Rubric may not discriminate performance levels
- **All-or-nothing** (only S/F): Rubric may be too coarse-grained

### Statistical Considerations

**Sample Size Impact**: Smaller sample sizes (e.g., n < 30) produce noisier distributions. The sample size annotation helps identify when distributions may be unreliable.

**Confidence Intervals**: For proportion $p$ with sample size $n$, the 95% confidence interval is approximately:

$$
p \pm 1.96 \sqrt{\frac{p(1-p)}{n}}
$$

Small proportions (e.g., S grade at 5%) have wider confidence intervals and may not be statistically different from zero.

## Visualization Details

### Chart Type

**Stacked Bar Chart** with normalized stacking (proportions sum to 1.0 per tier).

**Encoding Channels**:

- **X-axis**: Tier (categorical, ordered T0 → T6)
  - Label format: `"T0 (n=240)"` (tier + sample size)
  - Sorting: Natural tier order (T0 < T1 < ... < T6)
- **Y-axis**: Proportion (quantitative, 0.0 to 1.0)
  - Scale: Normalized (stack="normalize")
  - Title: "Proportion"
- **Color**: Grade (ordinal, ordered S → F)
  - Color scale: Custom palette from `config.yaml`
  - Ordering: Bottom-to-top stacking (F at bottom, S at top)
- **Facet**: Agent model (categorical)
  - Column-based faceting
  - Independent x-axis per facet (to accommodate different tier sets)

### Color Palette

Grades are assigned colors from `scylla/analysis/config.yaml`:

| Grade | Color | Hex Code | Semantic |
|-------|-------|----------|----------|
| S | Gold | `#FFD700` | Superior (perfect score) |
| A | Green | `#2ecc71` | Excellent (production ready) |
| B | Blue | `#3498db` | Good (meets requirements) |
| C | Orange | `#f39c12` | Acceptable (partial credit) |
| D | Dark Orange | `#e67e22` | Marginal (significant issues) |
| F | Red | `#e74c3c` | Failing (does not meet requirements) |

**Stacking Order**: Grades are stacked from bottom to top in reverse order (F → D → C → B → A → S) to ensure failing grades appear at the bottom and excellent grades at the top.

### Tooltip Information

Hovering over a bar segment displays:

- **Tier**: Testing tier (e.g., "T0")
- **Grade**: Letter grade (e.g., "A")
- **Count**: Number of runs with this grade
- **Total Runs**: Total runs in this tier
- **Proportion**: Percentage of runs with this grade (formatted as percentage)

Example: `Tier: T0 | Grade: A | Count: 45 | Total: 240 | Proportion: 18.75%`

### Layout

- **Title**: "Grade Distribution by Tier"
- **Faceting**: Column-based by `agent_model` (each model gets a separate panel)
- **X-axis resolution**: Independent per facet (allows different tier sets per model)
- **Aspect ratio**: Default Altair aspect ratio (adjusts to number of tiers)

## Interpretation Guidelines

### Reading the Figure

**Vertical Slices** (single tier, single model):

- Each bar represents 100% of runs in that tier
- Height of each colored segment = proportion of runs with that grade
- Taller segments at top (S/A/B) = better performance
- Taller segments at bottom (F/D) = more failures

**Horizontal Comparison** (across tiers, same model):

- Look for shifts in color distribution from left (T0) to right (T6)
- **Expected**: F/D segments shrink, S/A/B segments grow
- **Anomaly**: Regression (later tiers worse than earlier) or plateau (no change)

**Cross-Model Comparison** (same tier, different facets):

- Compare bar patterns across facets at the same tier
- Identify models with consistently better grade distributions
- Detect model-specific tier sensitivities

### Example Interpretations

**Scenario 1: Ideal Tier Progression**

```
T0: [F=60%, D=20%, C=10%, B=8%, A=2%, S=0%]
T3: [F=20%, D=15%, C=20%, B=30%, A=12%, S=3%]
T6: [F=5%, D=5%, C=15%, B=40%, A=30%, S=5%]
```

**Interpretation**: Clear improvement trajectory. T0 dominated by failures, T6 dominated by passing grades. System is working as expected.

**Scenario 2: Tier Regression**

```
T2: [F=30%, D=15%, C=20%, B=25%, A=8%, S=2%]
T3: [F=45%, D=20%, C=15%, B=15%, A=4%, S=1%]
```

**Interpretation**: T3 performs worse than T2 despite added capabilities. Investigate T3 tier definition for:

- Over-complex delegation patterns
- Incompatible tool combinations
- Increased latency causing timeouts

**Scenario 3: Bimodal Distribution**

```
T4: [F=35%, D=5%, C=5%, B=5%, A=10%, S=40%]
```

**Interpretation**: Sharp peaks at F and S, minimal mid-range grades. Suggests:

- Rubric has binary criteria (all-or-nothing)
- Test cases have high variance in difficulty
- Agent behavior is inconsistent (works perfectly or fails completely)

**Scenario 4: Plateau Effect**

```
T3: [F=15%, D=10%, C=20%, B=35%, A=15%, S=5%]
T4: [F=14%, D=10%, C=21%, B=34%, A=16%, S=5%]
T5: [F=15%, D=9%, C=20%, B=35%, A=16%, S=5%]
```

**Interpretation**: No meaningful improvement from T3 → T5. Suggests:

- Diminishing returns beyond T3
- T4/T5 capabilities don't address remaining failure modes
- May need different architectural approaches rather than incremental capabilities

### Common Patterns and Diagnoses

| Pattern | Description | Likely Cause | Action |
|---------|-------------|--------------|--------|
| **Monotonic improvement** | Each tier shows better distribution | Expected behavior | Continue to next phase |
| **Regression** | Later tier worse than earlier | Capability conflict | Investigate tier config |
| **Plateau** | No change across multiple tiers | Diminishing returns | Re-evaluate tier design |
| **Bimodal** | Peaks at F and S | Binary rubric criteria | Refine rubric gradations |
| **Uniform** | Flat distribution | Non-discriminating rubric | Add finer-grained criteria |
| **All F** | 100% failures | Fundamental failure mode | Check agent/tier setup |
| **All S** | 100% perfect scores | Task too easy / rubric too lenient | Increase difficulty |

### Statistical Significance

When comparing grade distributions across tiers or models, consider:

**Chi-squared test** for distribution equality:

- Null hypothesis: Grade distributions are identical
- Use when sample sizes are sufficient (n ≥ 30 per group)
- p < 0.05 indicates statistically significant difference

**Effect size**: Look for **practical significance** (large shifts in grade proportions) not just statistical significance. A shift from 60% F to 30% F is meaningful even if sample size is small.

## Related Figures

This figure complements other variance and performance analyses:

### Directly Related

- **Fig01: Score Variance by Tier** (`fig01_score_variance_by_tier`)
  - Shows continuous score distributions (histograms) rather than discrete grade categories
  - Provides finer-grained view of performance variance within each tier
  - Use together: Fig01 shows distribution shape, Fig03 shows grade categorization

### Failure Analysis

- **Fig18a: Failure Mode Distribution** (hypothetical reference from issue)
  - Breaks down F grades by failure type (timeout, error, incorrect output, etc.)
  - Complements Fig03 by explaining *why* failures occur

- **Fig18b: Failure Trends Over Time** (hypothetical reference from issue)
  - Shows failure rate evolution across experiment iterations
  - Complements Fig03 by showing temporal dimension

### Tier Performance

- **Fig05: Pass Rate by Tier** (if exists)
  - Shows binary pass/fail rather than full grade spectrum
  - Fig03 provides richer detail by showing grade distribution

- **Fig07: Cost of Pass by Tier** (if exists)
  - Economic view of tier performance
  - Use with Fig03 to correlate grade quality with cost efficiency

### Model Comparison

- **Fig10: Model Performance Comparison** (if exists)
  - Cross-model comparison across all tiers
  - Fig03 provides tier-specific breakdown via faceting

## Code Reference

### Function Signature

```python
def fig03_failure_rate_by_tier(
    runs_df: pd.DataFrame,
    output_dir: Path,
    render: bool = True
) -> None:
    """Generate Fig 3: Failure Rate by Tier.

    Stacked bar chart showing grade proportions.

    Args:
        runs_df: Runs DataFrame
        output_dir: Output directory
        render: Whether to render to PNG/PDF
    """
```

**Location**: `scylla/analysis/figures/variance.py:55-119`

### Implementation Steps

1. **Data Aggregation** (lines 69-75):

   ```python
   # Count runs per (agent_model, tier, grade)
   grade_counts = (
       runs_df.groupby(["agent_model", "tier", "grade"])
       .size()
       .reset_index(name="count")
   )

   # Calculate proportions (normalized stacking)
   grade_counts["total"] = grade_counts.groupby(["agent_model", "tier"])["count"].transform("sum")
   grade_counts["proportion"] = grade_counts["count"] / grade_counts["total"]
   ```

2. **Sample Size Annotation** (lines 78-80):

   ```python
   # Add sample size to tier labels: "T0 (n=240)"
   grade_counts["tier_label"] = grade_counts.apply(
       lambda row: f"{row['tier']} (n={int(row['total'])})", axis=1
   )
   ```

3. **Color Scale Configuration** (lines 82-86):

   ```python
   # Get canonical grade order (reversed for bottom-to-top stacking)
   grade_order = list(reversed(config.grade_order))

   # Get dynamic color scale from config
   domain, range_ = get_color_scale("grades", grade_order)
   ```

4. **Altair Chart Specification** (lines 89-117):

   ```python
   chart = (
       alt.Chart(grade_counts)
       .mark_bar()
       .encode(
           x=alt.X(
               "tier_label:N",
               title="Tier",
               sort=alt.EncodingSortField(field="tier", order="ascending"),
           ),
           y=alt.Y("proportion:Q", title="Proportion", stack="normalize"),
           color=alt.Color(
               "grade:O",
               title="Grade",
               sort=grade_order,
               scale=alt.Scale(domain=domain, range=range_),
           ),
           order=alt.Order("grade:O", sort="ascending"),
           tooltip=[...],
       )
       .facet(column=alt.Column("agent_model:N", title=None))
       .properties(title="Grade Distribution by Tier")
       .resolve_scale(x="independent")
   )
   ```

5. **Output** (line 119):

   ```python
   save_figure(chart, "fig03_failure_rate_by_tier", output_dir, render)
   ```

   Generates:
   - `fig03_failure_rate_by_tier.json` (Vega-Lite specification)
   - `fig03_failure_rate_by_tier.png` (if `render=True`)
   - `fig03_failure_rate_by_tier.pdf` (if `render=True`)

### Configuration Dependencies

- **Grade Order**: `config.grade_order` from `scylla/analysis/config.yaml`
  - Default: `["S", "A", "B", "C", "D", "F"]`
  - Reversed for stacking: `["F", "D", "C", "B", "A", "S"]`

- **Grade Colors**: `config.colors["grades"]` from `scylla/analysis/config.yaml`
  - See "Color Palette" section above for full mapping

- **Figure Saving**: `save_figure()` from `scylla.analysis.figures.spec_builder`
  - Handles JSON export and optional rendering to PNG/PDF

### Usage Example

```python
from pathlib import Path
import pandas as pd
from scylla.analysis.figures.variance import fig03_failure_rate_by_tier

# Load runs data
runs_df = pd.read_csv("results/runs.csv")

# Generate figure
output_dir = Path("figures/")
fig03_failure_rate_by_tier(runs_df, output_dir, render=True)

# Outputs:
# - figures/fig03_failure_rate_by_tier.json
# - figures/fig03_failure_rate_by_tier.png
# - figures/fig03_failure_rate_by_tier.pdf
```

### Testing

See test coverage in `tests/analysis/figures/test_variance.py` (if exists) for:

- Data aggregation correctness
- Proportion calculation validation
- Color scale consistency
- Edge cases (single grade, missing tiers, empty data)

---

**Version**: 1.0
**Last Updated**: 2026-02-12
**Maintained by**: ProjectScylla Analysis Team
