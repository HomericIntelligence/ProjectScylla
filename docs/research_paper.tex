\documentclass[11pt]{article}

% ====================================================================
% TEMPLATE NOTICE
% ====================================================================
% This is a TEMPLATE document for publication formatting demonstration.
% It contains HYPOTHETICAL DATA and does not reflect actual experimental
% results from ProjectScylla.
%
% ACTUAL EXPERIMENTAL DATA (as of 2026-01-31):
%   - 2 models (Sonnet 4.5-20250929, haiku)
%   - 226 total subtests across 7 tiers (T0-T6)
%   - Overall pass rate: 83.9% (not 91.3% Impl-Rate claimed below)
%   - See docs/data/summary.json for real statistics
%
% DO NOT USE THIS DOCUMENT AS A SOURCE OF TRUTH FOR RESEARCH FINDINGS.
% ====================================================================

% ----------------------------
% Packages
% ----------------------------
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{xcolor}

% ----------------------------
% Title Information
% ----------------------------
\title{Architectural Efficacy and Economic Sustainability of LLM Agents:\\
\large A Tiered Benchmarking Study Across Software Engineering Workflows}

\author{%
Project Odyssey Research Team\\
AI Systems Laboratory\\
\texttt{research@projectodyssey.ai}
}

\date{January 2025}

% ----------------------------
% Document
% ----------------------------
\begin{document}
\maketitle

% ----------------------------
% Abstract
% ----------------------------
\begin{abstract}
\textbf{[TEMPLATE: This abstract contains hypothetical data for formatting demonstration only.]}

We present a comprehensive benchmarking study of LLM-based agent architectures across seven incremental complexity tiers (T0--T6), evaluating performance, quality, and economic sustainability on 135 software engineering tasks spanning nine workflow categories. Using the Cost-of-Pass (CoP) framework, we quantify the marginal utility of architectural components including prompt engineering, domain skills, external tooling, flat delegation, and nested hierarchies. Our results reveal a critical ``Token Efficiency Chasm'' at T3, where external tool schemas increase token consumption by 340\% while improving efficacy by only 12\%. We identify T4 (Atomic Task Design) as the optimal tier for parallelizable workflows, achieving 54\% cost reduction and 72\% latency improvement. For complex, long-horizon tasks, T6 hybrid configurations combining Skills with Delegation achieve the best Frontier Cost-of-Pass, outperforming human baselines by 23\% in economic efficiency while maintaining 91.3\% Implementation Rate.
\end{abstract}

\vspace{1em}

\noindent\textbf{Keywords:} LLM agents; benchmarking; cost-of-pass; multi-agent systems; software engineering; agentic architectures

% ==================================================
\section{Summary}

\textbf{[TEMPLATE NOTE: Replace with actual experimental data. Current template claims 12 models and 135 tasks; actual data shows 2 models and 226 subtests.]}

This study establishes a rigorous framework for evaluating LLM agent architectures through the lens of economic sustainability. We benchmark 12 models across 7 complexity tiers on 135 software engineering tasks, introducing the Cost-of-Pass (CoP) metric as the primary economic indicator.

\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{Token Efficiency Chasm:} T3 (Tooling) consumes 340\% more tokens than T2 (Skills) with only 12\% efficacy improvement
    \item \textbf{Atomic Task Design:} T4 delegation reduces costs by 54\% and latency by 72\% on parallelizable tasks
    \item \textbf{Point of Diminishing Returns:} T5 hierarchies justify cost only for tasks requiring $>$3 verification cycles
    \item \textbf{Optimal Configuration:} T6 hybrid (Skills + Delegation + Agentic RAG) achieves best Frontier CoP
    \item \textbf{Economic Viability:} Best configurations outperform human labor costs by 23\% at 91.3\% Impl-Rate
\end{itemize}

% ==================================================
\section{Introduction}

The integration of large language models (LLMs) into autonomous software engineering systems has progressed rapidly, yet the field lacks rigorous frameworks for evaluating the economic sustainability of increasingly complex agent architectures. While sophisticated multi-agent systems promise improved efficacy, they introduce significant operational overhead that must be quantified against measurable performance gains.

\subsection{Problem Statement}
Traditional LLM evaluation focuses on final success rates, failing to capture the economic trade-offs inherent in production deployments. An agent that achieves 95\% accuracy at \$0.50 per task may be less economically viable than one achieving 85\% at \$0.08 per task, depending on the application context.

\subsection{Research Questions}
\begin{enumerate}
    \item What is the marginal utility of each incremental complexity tier (T0--T6)?
    \item Where does the ``Point of Diminishing Returns'' occur in architectural complexity?
    \item How does the Cost-of-Pass scale across different software engineering workflows?
    \item What hybrid configurations optimize the efficacy-to-cost ratio?
\end{enumerate}

\subsection{Contributions}
\begin{itemize}
    \item A comprehensive \textbf{Incremental Capability Matrix} (T0--T6) for ablative benchmarking
    \item The \textbf{Cost-of-Pass (CoP)} economic framework with component-level cost tracking
    \item Empirical quantification of the \textbf{Token Efficiency Chasm} between Skills and Tooling
    \item Validation of \textbf{Task-Contingent Coordination Laws} for multi-agent systems
    \item Actionable deployment guidance based on 135 real-world software engineering tasks
\end{itemize}

% ==================================================
\section{Related Work}

\subsection{LLM Agent Benchmarking}
Prior work on LLM benchmarking has focused primarily on capability assessment. AgentBoard introduced fine-grained progress metrics for multi-turn agents, while TheAgentCompany established benchmarks for real-world software tasks. Our work extends these frameworks by integrating economic sustainability metrics.

\subsection{Multi-Agent Architectures}
Research on multi-agent LLM systems has demonstrated both the promise and pitfalls of coordination. HockeyStack's production analysis showed 54\% cost reductions with atomic task design, while academic studies identified 39--70\% performance degradation on sequential tasks due to coordination overhead.

\subsection{Economic Frameworks}
The Cost-of-Pass framework, introduced for general LLM evaluation, provides the theoretical foundation for our economic analysis. We extend this framework to component-level cost tracking and multi-tier architectural comparison.

% ==================================================
\section{Test Methodology}

\subsection{Experimental Design}
We employ a controlled, sequential introduction of agentic capabilities, benchmarking performance and cost at each incremental step ($T(n)$ versus $T(n-1)$). This ablation methodology isolates the marginal contribution of each architectural component.

\subsection{Dimensional Search Space}
Our benchmark explores four primary dimensions:

\begin{itemize}
    \item \textbf{Agent Complexity:} Tiers 0--6 representing progressive architectural sophistication
    \item \textbf{Prompt Complexity:} Scale 0--10 from zero-shot to heavily engineered prompts
    \item \textbf{Skill Complexity:} Domain expertise categories (Build, Test, Deploy, Debug, Document)
    \item \textbf{Agent Hierarchy:} Flat (single-agent), Delegated (T4), Nested (T5), Hybrid (T6)
\end{itemize}

% ==================================================
\section{Test Metrics}

\subsection{Performance Metrics}
\begin{itemize}
    \item \textbf{Full Completion Score ($S_{Full}$):} Binary task completion measure
    \item \textbf{Fine-Grained Progress Rate ($R_{Prog}$):} Incremental advancement tracking (0.0--1.0)
    \item \textbf{Latency:} Time from query submission to task resolution (seconds)
    \item \textbf{Consistency:} Output stability coefficient across identical inputs (0.0--1.0)
\end{itemize}

\subsection{Quality Metrics}
\begin{itemize}
    \item \textbf{Pass-Rate:} Automated test suite pass percentage
    \item \textbf{Implementation Rate (Impl-Rate):} LLM-as-Judge requirement verification (0.0--1.0)
    \item \textbf{Change Fail Percentage (CFP):} Production changes causing failures (\%)
    \item \textbf{PR Revert Rate:} Human-rejected agent-generated code (\%)
\end{itemize}

\subsection{Efficiency and Cost Metrics}
The Cost-of-Pass (CoP) integrates cost with accuracy:

\begin{equation}
    CoP = \frac{\text{Expected Cost}}{R_m(p)}
\end{equation}

where $R_m(p)$ is the accuracy on problem $p$. When accuracy approaches zero, CoP approaches infinity, signaling economic infeasibility.

Additional metrics include:
\begin{itemize}
    \item \textbf{Frontier CoP:} $\min(CoP)$ across all configurations
    \item \textbf{Token Efficiency:} Useful output per input token
    \item \textbf{Component Cost Breakdown:} Per-module cost attribution
\end{itemize}

% ==================================================
\section{Test Configuration}

\subsection{Hardware and Infrastructure}
All experiments were conducted on:
\begin{itemize}
    \item Cloud compute: AWS p4d.24xlarge instances (8Ã— NVIDIA A100 40GB)
    \item API endpoints: Direct provider APIs with consistent rate limiting
    \item Orchestration: Custom evaluation harness with deterministic seeding
\end{itemize}

\subsection{Software Stack}
\begin{itemize}
    \item Orchestration framework: LangGraph 0.2.x with custom coordination layer
    \item Evaluation harness: PyTest-based with LLM-as-Judge validation
    \item Cost tracking: Real-time token metering with provider-specific pricing
\end{itemize}

\subsection{Model Configuration}
All models evaluated with:
\begin{itemize}
    \item Temperature: 0.1 (deterministic mode)
    \item Max tokens: 8,192 (output limit)
    \item Context: Model-specific maximum
\end{itemize}

% ==================================================
\section{Test Cases}

\subsection{Pull Request Selection}
We selected 5 representative PRs for each combination of workflow category and size tier, totaling 135 test cases per complexity tier.

\subsubsection{PR Size Categories}
\begin{itemize}
    \item \textbf{Small:} $<$100 lines of code (45 PRs)
    \item \textbf{Medium:} 300--500 lines of code (45 PRs)
    \item \textbf{Large:} 500--2000 lines of code (45 PRs)
\end{itemize}

\subsection{Workflow Categories}
\begin{itemize}
    \item \textbf{Build System:} Build configuration, dependency management, compilation optimization
    \item \textbf{CI/CD:} Pipeline configuration, deployment automation, environment setup
    \item \textbf{Bug Fixing:} Defect resolution, regression fixes, error handling improvements
    \item \textbf{New Features:} Feature implementation, API additions, capability extensions
    \item \textbf{Refactoring:} Code restructuring, pattern application, technical debt reduction
    \item \textbf{Optimization:} Performance improvements, resource efficiency, caching
    \item \textbf{Review:} Code review suggestions, PR feedback, style corrections
    \item \textbf{Documentation:} Doc generation, comment updates, README improvements
    \item \textbf{Issue Filing:} Bug reports, feature requests, reproduction cases
\end{itemize}

\subsection{Test Case Matrix}
Table~\ref{tab:test-matrix} shows the distribution of test cases across workflows and sizes.

\begin{table}[H]
\centering
\caption{Test Case Distribution by Workflow and Size}
\label{tab:test-matrix}
\begin{tabular}{lcccr}
\toprule
\textbf{Workflow} & \textbf{Small} & \textbf{Medium} & \textbf{Large} & \textbf{Total} \\
\midrule
Build System & 5 & 5 & 5 & 15 \\
CI/CD & 5 & 5 & 5 & 15 \\
Bug Fixing & 5 & 5 & 5 & 15 \\
New Features & 5 & 5 & 5 & 15 \\
Refactoring & 5 & 5 & 5 & 15 \\
Optimization & 5 & 5 & 5 & 15 \\
Review & 5 & 5 & 5 & 15 \\
Documentation & 5 & 5 & 5 & 15 \\
Issue Filing & 5 & 5 & 5 & 15 \\
\midrule
\textbf{Total} & 45 & 45 & 45 & \textbf{135} \\
\bottomrule
\end{tabular}
\end{table}

% ==================================================
\section{Model Summary}

\subsection{Claude Code}
\begin{itemize}
    \item \textbf{Claude Opus:} Strategic orchestration, maximum reasoning depth, 200K context
    \item \textbf{Claude Sonnet:} Balanced execution, cost-effective quality, 200K context
    \item \textbf{Claude Haiku:} High-throughput utility, fast response, 200K context
\end{itemize}

\subsection{OpenAI}
\begin{itemize}
    \item \textbf{GPT-5.2 / Codex:} Code-specialized reasoning, extended tool use, 128K context
\end{itemize}

\subsection{Large Model CLI-Based Systems}
All models evaluated in standardized CLI-based agentic workflows:
\begin{itemize}
    \item Claude Opus (Anthropic)
    \item OpenAI GPT-5.2 (OpenAI)
    \item Gemini 3.0 Pro (Google)
    \item DeepSeek-V3 (DeepSeek AI)
    \item Qwen 3-72B (Alibaba)
    \item MBZ-K2 (Technology Innovation Institute)
    \item Kimi-K2 (Moonshot AI)
    \item Kimi-3 (Moonshot AI)
\end{itemize}

Table~\ref{tab:model-specs} summarizes model specifications and pricing.

\begin{table}[H]
\centering
\caption{Model Specifications and Pricing (per 1M tokens)}
\label{tab:model-specs}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Model} & \textbf{Context} & \textbf{Input \$} & \textbf{Output \$} & \textbf{Latency (s)} \\
\midrule
Claude Opus & 200K & 15.00 & 75.00 & 2.8 \\
Claude Sonnet & 200K & 3.00 & 15.00 & 1.4 \\
Claude Haiku & 200K & 0.25 & 1.25 & 0.3 \\
GPT-5.2 & 128K & 10.00 & 30.00 & 1.8 \\
Gemini 3.0 Pro & 1M & 1.25 & 5.00 & 1.1 \\
DeepSeek-V3 & 64K & 0.14 & 0.28 & 0.8 \\
Qwen 3-72B & 128K & 0.40 & 0.40 & 1.2 \\
MBZ-K2 & 128K & 0.20 & 0.20 & 1.5 \\
Kimi-K2 & 200K & 0.60 & 2.00 & 1.3 \\
Kimi-3 & 200K & 0.90 & 3.00 & 1.1 \\
\bottomrule
\end{tabular}
\end{table}

% ==================================================
\section{Results}

\subsection{Quantitative Results}

Table~\ref{tab:tier-results} presents the aggregate performance across complexity tiers.

\begin{table}[H]
\centering
\caption{Performance by Complexity Tier (All Models Averaged)}
\label{tab:tier-results}
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Tier} & \textbf{Impl-Rate} & \textbf{Pass-Rate} & \textbf{$R_{Prog}$} & \textbf{Latency} & \textbf{Tokens} & \textbf{CoP} \\
\midrule
T0 (Vanilla) & 0.342 & 0.287 & 0.41 & 1.2s & 2,847 & \$0.089 \\
T1 (Prompted) & 0.567 & 0.498 & 0.62 & 1.8s & 4,231 & \$0.047 \\
T2 (Skills) & 0.734 & 0.681 & 0.78 & 2.4s & 6,892 & \$0.031 \\
T3 (Tooling) & 0.821 & 0.756 & 0.84 & 4.7s & 23,412 & \$0.098 \\
T4 (Delegation) & 0.867 & 0.812 & 0.89 & 3.1s & 18,234 & \$0.042 \\
T5 (Hierarchy) & 0.912 & 0.867 & 0.93 & 8.4s & 47,891 & \$0.127 \\
T6 (Hybrid) & 0.913 & 0.871 & 0.94 & 4.2s & 21,456 & \$0.034 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{The Token Efficiency Chasm}
Figure~\ref{fig:fig07_token_distribution} illustrates the dramatic token consumption increase at T3:
\begin{itemize}
    \item T2 $\rightarrow$ T3 token increase: \textbf{+340\%} (6,892 $\rightarrow$ 23,412)
    \item T2 $\rightarrow$ T3 Impl-Rate increase: \textbf{+12\%} (0.734 $\rightarrow$ 0.821)
    \item CoP degradation: \textbf{+216\%} (\$0.031 $\rightarrow$ \$0.098)
\end{itemize}

\subsection{Comparative Analysis}

Table~\ref{tab:model-comparison} shows per-model performance at the optimal T6 tier.

\begin{table}[H]
\centering
\caption{Model Comparison at T6 (Hybrid Configuration)}
\label{tab:model-comparison}
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Model} & \textbf{Impl-Rate} & \textbf{Pass-Rate} & \textbf{CFP} & \textbf{Revert} & \textbf{CoP} \\
\midrule
Claude Opus & 0.947 & 0.912 & 2.1\% & 3.2\% & \$0.089 \\
Claude Sonnet & 0.923 & 0.889 & 2.8\% & 4.1\% & \$0.038 \\
Claude Haiku & 0.812 & 0.767 & 5.4\% & 7.8\% & \$0.012 \\
GPT-5.2 & 0.934 & 0.898 & 2.4\% & 3.7\% & \$0.067 \\
Gemini 3.0 Pro & 0.891 & 0.845 & 3.7\% & 5.2\% & \$0.024 \\
DeepSeek-V3 & 0.867 & 0.823 & 4.2\% & 6.1\% & \$0.008 \\
Qwen 3-72B & 0.845 & 0.798 & 4.8\% & 6.9\% & \$0.014 \\
MBZ-K2 & 0.823 & 0.778 & 5.1\% & 7.3\% & \$0.011 \\
Kimi-K2 & 0.856 & 0.812 & 4.4\% & 6.4\% & \$0.019 \\
Kimi-3 & 0.878 & 0.834 & 3.9\% & 5.6\% & \$0.023 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Workflow-Specific Performance}
Table~\ref{tab:workflow-results} shows optimal tier by workflow category.

\begin{table}[H]
\centering
\caption{Optimal Tier and Performance by Workflow Category}
\label{tab:workflow-results}
\begin{tabular}{lcrrr}
\toprule
\textbf{Workflow} & \textbf{Optimal Tier} & \textbf{Impl-Rate} & \textbf{CoP} & \textbf{Rationale} \\
\midrule
Build System & T4 & 0.891 & \$0.034 & Parallelizable subtasks \\
CI/CD & T3/T4 & 0.867 & \$0.067 & External tool integration \\
Bug Fixing & T5 & 0.923 & \$0.098 & Verification critical \\
New Features & T6 & 0.912 & \$0.045 & Long-horizon planning \\
Refactoring & T4 & 0.878 & \$0.031 & Atomic changes \\
Optimization & T4 & 0.845 & \$0.038 & Measurable improvements \\
Review & T2 & 0.856 & \$0.019 & Judgment-based \\
Documentation & T2 & 0.889 & \$0.014 & Token-efficient \\
Issue Filing & T2 & 0.867 & \$0.011 & Judgment-based \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cost--Performance Trade-offs}

\subsubsection{Scaling Behavior by PR Size}
Table~\ref{tab:size-scaling} reveals how CoP scales with task complexity.

\begin{table}[H]
\centering
\caption{CoP Scaling by PR Size (T6 Hybrid, Claude Sonnet)}
\label{tab:size-scaling}
\begin{tabular}{lrrrr}
\toprule
\textbf{PR Size} & \textbf{Impl-Rate} & \textbf{Tokens} & \textbf{Cost} & \textbf{CoP} \\
\midrule
Small ($<$100 LOC) & 0.956 & 12,345 & \$0.024 & \$0.025 \\
Medium (300--500 LOC) & 0.912 & 24,567 & \$0.048 & \$0.053 \\
Large (500--2000 LOC) & 0.834 & 48,923 & \$0.096 & \$0.115 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Diminishing Returns Analysis}
The Point of Diminishing Returns occurs at T5 for most workflows:
\begin{itemize}
    \item T4 $\rightarrow$ T5 Impl-Rate gain: +5.2\% (0.867 $\rightarrow$ 0.912)
    \item T4 $\rightarrow$ T5 CoP increase: +202\% (\$0.042 $\rightarrow$ \$0.127)
    \item T5 justified only when CFP $<$ 3\% is required
\end{itemize}

% ==================================================
\section{Discussion}

\subsection{Interpretation of Results}
The Token Efficiency Chasm at T3 represents a critical architectural decision point. While external tooling enables real-world execution capabilities, the schema overhead of loading comprehensive tool libraries ``just in case'' results in dramatic cost increases with marginal efficacy gains.

T4's Atomic Task Design emerges as the optimal strategy for most parallelizable workflows, validating the hypothesis that specialized, narrow agents outperform generalist approaches in both cost and quality metrics.

\subsection{Failure Modes and Limitations}
\begin{itemize}
    \item \textbf{Sequential Task Degradation:} Multi-agent configurations degrade by 39--70\% on rigid sequential tasks
    \item \textbf{Context Drift:} T5 hierarchies exhibit strategic drift after 7+ delegation cycles
    \item \textbf{Capability Saturation:} Coordination provides no benefit when single-agent baseline exceeds 45\%
\end{itemize}

\subsection{Implications for Agent Design}
\begin{enumerate}
    \item Prefer Skills (T2) over Tooling (T3) for judgment-based tasks
    \item Use Atomic Task Design (T4) for parallelizable workflows
    \item Reserve T5 hierarchies for verification-critical long-horizon tasks
    \item Deploy T6 hybrid configurations for optimal economic sustainability
\end{enumerate}

% ==================================================
\section{Conclusions}

This study provides definitive evidence for the economic sustainability of LLM agent architectures through the Cost-of-Pass framework.

\textbf{Key Findings:}
\begin{enumerate}
    \item The Token Efficiency Chasm at T3 represents a 216\% CoP increase for only 12\% efficacy gain
    \item T4 Atomic Task Design achieves 54\% cost reduction with 5.2\% efficacy improvement
    \item T6 Hybrid configurations achieve the lowest Frontier CoP (\$0.034) at 91.3\% Impl-Rate
    \item Optimal tier selection is task-contingent, with judgment tasks favoring T2 and complex tasks requiring T5/T6
\end{enumerate}

The economic viability threshold is achieved when agent CoP falls below human labor costs. At \$0.034 per task with 91.3\% success, T6 configurations outperform estimated human costs (\$0.044 equivalent) by 23\%.

% ==================================================
\section{Further Work}

\begin{itemize}
    \item \textbf{Statistical Framework:} Apply Hierarchical Bayesian Models (HiBayES) for robust multi-level analysis
    \item \textbf{Dynamic Tier Selection:} Develop automated classifiers for optimal tier assignment
    \item \textbf{Model Distillation:} Create specialized Monitor/Critic models to reduce T5 costs
    \item \textbf{Adaptive Tool Loading:} Implement lazy schema loading to mitigate T3 overhead
    \item \textbf{Extended Domains:} Validate findings across non-software engineering tasks
\end{itemize}

% ==================================================
\section*{Acknowledgements}
This research was supported by the Project Odyssey initiative. We thank the open-source community for benchmark contributions and the model providers for API access.

% ==================================================
\bibliographystyle{plain}
% \bibliography{references}

% ==================================================
\appendix

\section{Detailed Metric Definitions}

\subsection{Cost-of-Pass (CoP)}
\begin{equation}
    CoP(m, p) = \frac{C(m, p)}{R_m(p)}
\end{equation}
where $C(m, p)$ is the cost of model $m$ on problem $p$, and $R_m(p)$ is the accuracy.

\subsection{Fine-Grained Progress Rate}
\begin{equation}
    R_{Prog} = \frac{1}{N} \sum_{i=1}^{N} \frac{\text{Milestones Achieved}_i}{\text{Total Milestones}_i}
\end{equation}

\subsection{Change Fail Percentage}
\begin{equation}
    CFP = \frac{\text{Failed Deployments}}{\text{Total Deployments}} \times 100\%
\end{equation}

\section{Additional Tables and Figures}

\begin{table}[H]
\centering
\caption{Complete Tier Performance Matrix (All Models)}
\label{tab:full-matrix}
\begin{tabular}{llrrrr}
\toprule
\textbf{Model} & \textbf{Tier} & \textbf{Impl-Rate} & \textbf{Tokens} & \textbf{Latency} & \textbf{CoP} \\
\midrule
Claude Opus & T0 & 0.423 & 3,124 & 2.1s & \$0.112 \\
Claude Opus & T2 & 0.812 & 8,456 & 3.2s & \$0.056 \\
Claude Opus & T4 & 0.912 & 21,345 & 4.1s & \$0.067 \\
Claude Opus & T6 & 0.947 & 25,678 & 4.8s & \$0.089 \\
\midrule
Claude Sonnet & T0 & 0.389 & 2,987 & 1.4s & \$0.048 \\
Claude Sonnet & T2 & 0.778 & 7,234 & 2.1s & \$0.028 \\
Claude Sonnet & T4 & 0.889 & 18,456 & 2.8s & \$0.034 \\
Claude Sonnet & T6 & 0.923 & 21,234 & 3.4s & \$0.038 \\
\midrule
DeepSeek-V3 & T0 & 0.312 & 2,456 & 0.6s & \$0.004 \\
DeepSeek-V3 & T2 & 0.689 & 5,678 & 0.9s & \$0.005 \\
DeepSeek-V3 & T4 & 0.823 & 14,567 & 1.4s & \$0.006 \\
DeepSeek-V3 & T6 & 0.867 & 17,890 & 1.8s & \$0.008 \\
\bottomrule
\end{tabular}
\end{table}

\input{figures/fig07_token_distribution_include.tex}
\section{Reproducibility Checklist}

\begin{itemize}
    \item[$\square$] Model versions and API endpoints documented
    \item[$\square$] Random seeds fixed for deterministic evaluation
    \item[$\square$] Token counts verified via provider APIs
    \item[$\square$] Cost calculations based on published pricing
    \item[$\square$] Evaluation harness available at \texttt{github.com/project-odyssey/benchmark}
    \item[$\square$] Test case dataset available upon request
\end{itemize}

\end{document}
