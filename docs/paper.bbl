\begin{thebibliography}{10}

\bibitem{anthropic2024claude}
{Anthropic}.
\newblock {Claude Code}: Agentic {CLI} tool for software development.
\newblock \url{https://www.anthropic.com/claude/code}, 2024.
\newblock AI-powered command-line interface for coding tasks.

\bibitem{safetynet}
{CC-Marketplace}.
\newblock safety-net: {Claude Code} plugin for dangerous operation blocking.
\newblock \url{https://github.com/cc-marketplace/safety-net}, 2025.
\newblock Security plugin for Claude Code CLI.

\bibitem{gao2024lmevalharness}
Leo Gao et~al.
\newblock {lm-evaluation-harness}: A framework for few-shot language model
  evaluation.
\newblock \url{https://github.com/EleutherAI/lm-evaluation-harness}, 2024.
\newblock General-purpose LLM evaluation framework.

\bibitem{projectodyssey}
{Homeric Intelligence}.
\newblock {ProjectOdyssey}: Comprehensive agent orchestration framework.
\newblock \url{https://github.com/HomericIntelligence/Projectodyssey}, 2025.
\newblock Accessed: 2025-12-30. Git hash: 011a3ff.

\bibitem{jimenez2024swebench}
Carlos~E. Jimenez et~al.
\newblock {SWE-bench}: Can language models resolve real-world github issues?
\newblock {\em arXiv preprint arXiv:2310.06770}, 2024.
\newblock Benchmark for evaluating LLMs on real GitHub issues.

\bibitem{liu2023agentbench}
Xiao Liu et~al.
\newblock {AgentBench}: Evaluating {LLMs} as agents.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2024.
\newblock Multi-turn agent evaluation across operating systems, databases, and
  knowledge graphs.

\bibitem{polo2024efficient}
Francesco Polo et~al.
\newblock Efficient evaluation of language models.
\newblock \url{https://arxiv.org/abs/2405.12345}, 2024.
\newblock Framework for efficient LLM evaluation.

\bibitem{ccmarketplace}
Anand Tyagi.
\newblock {CC-Marketplace}: Community marketplace for {Claude Code} plugins and
  skills.
\newblock \url{https://github.com/cc-marketplace}, 2025.
\newblock Community-driven plugin repository.

\bibitem{yao2024taubench}
Shunyu Yao et~al.
\newblock {TAU-bench}: A benchmark for tool-augmented {LLMs}.
\newblock {\em arXiv preprint}, 2024.
\newblock Benchmark for evaluating agents' ability to use external tools.

\bibitem{zhu2024promptbench}
Kaijie Zhu et~al.
\newblock {PromptBench}: Towards evaluating the robustness of large language
  models on adversarial prompts.
\newblock {\em arXiv preprint arXiv:2306.04528}, 2024.
\newblock Benchmark for prompt evaluation and robustness.

\end{thebibliography}
