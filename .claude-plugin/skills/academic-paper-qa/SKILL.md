# Academic Paper QA: LaTeX Validation & Data Accuracy

## Overview

| Field | Value |
|-------|-------|
| **Date** | 2026-02-06 |
| **Objective** | Fix data accuracy issues, improve structure, and validate quantitative claims in academic LaTeX papers |
| **Outcome** | ✅ Successfully validated and corrected 8 critical data errors, removed 5 low-information figures, consolidated redundant sections, and achieved clean compilation with automated arXiv build |
| **Category** | Documentation |
| **Models Used** | Sonnet 4.5 |
| **Tools** | pdflatex, LaTeX, Python build scripts, grep, sed |

## When to Use This Skill

Use this skill when:

1. **Preparing academic papers for submission** with quantitative results that need validation
2. **Reviewing papers** where claims don't match source data (e.g., "scores 0.95-1.00" but actual data shows all 1.00)
3. **Fixing LaTeX compilation errors** related to Unicode characters, table formatting, or path issues
4. **Building arXiv submission packages** from source LaTeX with automated transformations
5. **Converting Markdown content** to proper LaTeX format in academic papers
6. **Consolidating redundant sections** (Abstract/Summary/Introduction overlap)
7. **Debugging failed LaTeX builds** with cryptic `\hline` or `\noalign` errors

**Red flags that indicate you need this skill**:
- Paper contains phrases like "approximately", "~", or broad ranges where precise values exist
- Figures show "no variance" or "all zeros" but are still included
- Same content appears in multiple sections (Abstract + Summary + Introduction)
- Build script warnings about missing sections or alignment issues
- Unicode characters (ρ, Δ, α, μ, σ) in LaTeX source instead of math mode
- Table compilation errors with column count mismatches

## Verified Workflow

### Phase 1: Data Validation (Read Source Data First)

**CRITICAL**: Always validate quantitative claims against source data files before accepting paper claims.

```bash
# 1. Read the paper to identify quantitative claims
# Look for: percentages, ranges, correlations, scores, statistics

# 2. Locate source data files
find . -name "*.csv" -o -name "*.json" -o -name "result.json"

# 3. Cross-reference each claim
# Example: Paper says "scores 0.95-1.00" → Check actual CSV values
# Example: Paper says "correlations >0.85" → Verify Pearson/Spearman values
```

**Common Data Accuracy Issues Found**:
- Broad ranges used when all values are identical (e.g., "0.95-1.00" when all are 1.00)
- Rounding that obscures precision (e.g., "25-41 seconds" when actual is 24.8-41.2s)
- Percentage claims that don't match actual data (e.g., "80-99%" when actual is 79.3-83.1%)
- Correlation strength claims that contradict values (e.g., ">0.85" when actual is 0.333/-0.273/-0.522)

### Phase 2: Structural Cleanup

**Remove Low-Information Content**:

```bash
# Identify figures/tables that show:
# - All zeros (failure rates when pass rate = 100%)
# - All ones (pass rates at ceiling)
# - Flat lines (tier uplift when all tiers perform equally)
# - Negligible effect sizes (with N=1 dryrun)

# Example removal with explanation:
# Remove Fig 3 (failure_rate) - all zeros in N=1 dryrun
# Remove Fig 4 (pass_rate) - all 1.0, no discrimination
# Add note: "Additional diagnostic figures available in repository but show no variance"
```

**Consolidate Redundant Sections**:

Pattern: Abstract + Summary + Introduction often overlap heavily in papers.

Fix:
1. Identify unique content in Summary not in Abstract
2. Merge unique content into Introduction
3. Remove Summary section entirely
4. Result: Abstract (high-level) → Introduction (detailed context)

### Phase 3: LaTeX Compilation Fixes

**Fix Unicode Characters**:

```latex
% WRONG (will fail compilation):
Spearman ρ = 0.333, mean Δ = 0.033, Krippendorff's α = -0.117

% CORRECT (use LaTeX math):
Spearman $\rho$ = 0.333, mean $\Delta$ = 0.033, Krippendorff's $\alpha$ = -0.117
```

**Fix Table Column Mismatches**:

```latex
% ERROR: 5 columns declared |l|l|l|l|l| but only 4 in header
\begin{tabular}{|l|l|l|l|l|}  % <-- 5 columns
\hline
Tier & Agent Time & Judge Time & Total Time & Judge % \\  % <-- only 4!
\hline

% FIX: Match column count (use right-align for numbers)
\begin{tabular}{|l|r|r|r|r|}  % <-- 5 columns, right-align numbers
\hline
Tier & Agent Time (s) & Judge Time (s) & Total Time (s) & Judge \% of Total \\
\hline
```

**Escape Underscores in Auto-Generated Tables**:

```latex
% Auto-generated tables often have unescaped underscores:
code_quality & 0.20 & 1.000 $\pm$ 0.000 \\  % <-- FAILS

% Fix with backslash escaping:
code\_quality & 0.20 & 1.000 $\pm$ 0.000 \\  % <-- WORKS
```

### Phase 4: Path Transformations for ArXiv Builds

**Problem**: Source uses `docs/paper-dryrun/figures/` but arXiv build needs `figures/`

**Solution**: Add path transformations to build script:

```python
# In build_arxiv_paper.py or transformation script:
def fix_relative_paths(content: str) -> str:
    """Transform paths for arXiv submission."""
    # Fix docs/paper-dryrun/ prefixes
    content = re.sub(r"docs/paper-dryrun/figures/", "figures/", content)
    content = re.sub(r"docs/paper-dryrun/tables/", "tables/", content)

    # ALSO fix bare paper-dryrun/ prefixes (commonly missed!)
    content = re.sub(r"paper-dryrun/figures/", "figures/", content)
    content = re.sub(r"paper-dryrun/tables/", "tables/", content)

    return content
```

### Phase 5: Establish Single Source of Truth

**Problem**: Build scripts check alignment between `paper.md` and `paper.tex`, causing false warnings when paper.tex becomes authoritative.

**Solution**: Update verification to skip outdated checks:

```python
# In verify_paper_alignment.py:
if __name__ == "__main__":
    # paper.tex is now the single source of truth
    # Skip alignment verification with paper.md
    print("ℹ Note: paper.tex is the source of truth (paper.md is deprecated)")
    print("✓ Verification skipped - paper.tex is canonical")
    sys.exit(0)
```

## Failed Attempts & Lessons Learned

### ❌ Attempt 1: Trust Paper Claims Without Verification
**What we tried**: Initially accepted paper's quantitative claims as accurate.

**Why it failed**: Paper contained 8+ data accuracy errors:
- Figure 14: Claimed "correlations >0.85" but actual was 0.333/-0.273/-0.522
- Functional scores: Claimed "0.95-1.00" but all were exactly 1.00
- Cache read %: Claimed "80-99%" but actual was 79.3-83.1%

**Lesson**: ALWAYS validate quantitative claims against source data files. Never trust ranges or approximations without checking raw data.

### ❌ Attempt 2: Compile LaTeX Before Fixing Unicode
**What we tried**: Attempted to compile paper.tex with Unicode characters (ρ, Δ, α) directly in source.

**Error**:
```
! LaTeX Error: Unicode character ρ (U+03C1) not set up for use with LaTeX.
! LaTeX Error: Unicode character α (U+03B1) not set up for use with LaTeX.
```

**Why it failed**: Standard LaTeX requires math mode for Greek letters.

**Lesson**: Search for Unicode characters `grep -n "[ρΔαμσ]" file.tex` and replace with LaTeX equivalents `$\rho$`, `$\Delta$`, `$\alpha$` BEFORE first compilation attempt.

### ❌ Attempt 3: Assume Auto-Generated Tables Are LaTeX-Safe
**What we tried**: Used table files generated from CSV with underscores in criterion names.

**Error**:
```
! Missing $ inserted.
<inserted text>
                $
l.10 code_quality & 0.20 & 1.000 $\pm$ 0.000 & --- & --- \\
```

**Why it failed**: Underscores are special characters in LaTeX (subscript in math mode) and must be escaped.

**Lesson**: Run `scripts/fix_table_underscores.py` on all auto-generated tables, or add escaping to the generation script. Never assume CSV → LaTeX conversion is LaTeX-safe.

### ❌ Attempt 4: Incomplete Path Transformation
**What we tried**: Added transformation for `docs/paper-dryrun/` but missed bare `paper-dryrun/` prefix.

**Error**:
```
! LaTeX Error: File `paper-dryrun/tables/tab04_criteria_performance.tex' not found.
```

**Why it failed**: Path transformation only caught `docs/paper-dryrun/` pattern, but manual `\input{paper-dryrun/tables/...}` used shorter form.

**Lesson**: When adding path transformations, search for ALL possible path prefixes:
```bash
grep -r "paper-dryrun" docs/paper.tex  # Find all patterns
# Then add regex for each: r"paper-dryrun/tables/" AND r"docs/paper-dryrun/tables/"
```

### ❌ Attempt 5: Keep All Generated Figures
**What we tried**: Included all 26 generated figures in appendix, even those showing no variance.

**Why it failed**: With N=1 dryrun:
- Fig 3 (failure_rate): All zeros (100% pass rate)
- Fig 4 (pass_rate): All 1.0 (ceiling effect)
- Fig 11 (tier_uplift): Flat lines (no improvement)
- Fig 18, 19: No discriminatory power

**Lesson**: Review each figure for information content. If figure shows "no variance" or "insufficient N", remove it and add explanatory note: "Additional diagnostic figures available in repository but show no variance in this N=1 dryrun."

## Results & Parameters

### LaTeX Compilation
```bash
# Successful compilation workflow:
cd docs/
pdflatex -interaction=nonstopmode paper.tex  # Pass 1
pdflatex -interaction=nonstopmode paper.tex  # Pass 2 (cross-refs)

# Output: paper.pdf (33 pages, 5.77 MB)
```

### ArXiv Build
```bash
# Automated build with verification:
bash scripts/build_arxiv_submission.sh

# Output:
# ✓ main.tex generated (34 pages, 663 KB PDF)
# ✓ submission.tar.gz created (4.77 MB)
# ✓ All 24 figures copied
# ✓ All 10 tables copied with underscore escaping
```

### Data Validation Results
| Issue | Paper Claim | Actual Data | Fixed To |
|-------|-------------|-------------|----------|
| Fig 14 correlation | ">0.85" | 0.333/-0.273/-0.522 (Spearman), 0.706/-0.063/-0.347 (Pearson) | "Low-to-moderate correlations, Opus-Sonnet highest (r=0.706)" |
| Functional scores | "0.95-1.00" | All exactly 1.00 | "Perfect functional scores (1.00)" |
| Build pipeline | "0.90-1.00" | All exactly 1.00 | "All tiers score 1.00" |
| Cache read % | "80-99%" | 79.3-83.1% | "~79-83%" |
| Agent time | "25-41 seconds" | 24.8-41.2s | "24.8-41.2 seconds" |

### File Changes Summary
```
Modified files:
- docs/paper.tex (source of truth)
- docs/paper-dryrun/tables/tab04_criteria_performance.tex (escaped underscores)
- scripts/build_arxiv_paper.py (enhanced path transformations)
- scripts/verify_paper_alignment.py (skip paper.md check)

Removed sections:
- Section 1: Keywords (converted to unnumbered paragraph)
- Section 2: Summary (merged into Introduction)
- Section 9: Model Summary (merged into Section 7.3)
- Section 10.7: Statistical Limitations (merged into 11.4)

Removed figures:
- Fig 3, 4, 11, 18, 19 (zero-information with N=1)
```

## Key Takeaways

1. **Data validation is non-negotiable**: Read source CSVs/JSONs before accepting any quantitative claim
2. **Unicode → LaTeX math**: Always convert ρ, Δ, α, μ, σ to `$\rho$`, `$\Delta$`, etc.
3. **Escape underscores**: Auto-generated tables need `\_` for criterion names like `code_quality`
4. **Table column counts**: Match `\begin{tabular}{|l|r|r|r|}` column spec with header exactly
5. **Path transformations**: Cover ALL path prefix variations (`docs/paper-dryrun/` AND `paper-dryrun/`)
6. **Remove zero-information content**: Figures/tables that show no variance dilute the message
7. **Consolidate redundancy**: Abstract + Summary + Introduction often overlap heavily
8. **Single source of truth**: Pick one (paper.tex or paper.md) and update build scripts accordingly

## References

- LaTeX compilation errors: https://www.overleaf.com/learn/latex/Errors
- Booktabs package for tables: https://ctan.org/pkg/booktabs
- ArXiv submission guide: https://arxiv.org/help/submit_tex
- Data validation best practices: Verify quantitative claims against source data files
