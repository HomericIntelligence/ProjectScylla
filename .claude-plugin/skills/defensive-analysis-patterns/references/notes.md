# Session Notes: Defensive Analysis Patterns

## Raw Session Details

**Date**: 2026-02-01
**Session ID**: a8bcc1e9-cead-44e0-abe7-195709826d9e
**User Request**: "fix the P2/P3 issues and commit all the changes as PR's"

## Full Timeline

### Phase 1: P2-3 Config Centralization (First)

**Files Modified**:
- `src/scylla/analysis/config.yaml` (+7 lines)
- `src/scylla/analysis/config.py` (+14 lines)
- `src/scylla/analysis/figures/correlation.py` (-6 +9 lines)

**Commit**: `c8cb23a`
**PR**: #307
**Status**: Merged

### Phase 2: P2-1 Parametrized Tests

**Files Created**:
- `tests/unit/analysis/test_stats_parametrized.py` (+356 lines)

**Test Classes**:
1. `TestCliffsDeltaParametrized` (17 tests)
2. `TestBootstrapCIParametrized` (11 tests)
3. `TestMannWhitneyUParametrized` (7 tests)
4. `TestConsistencyParametrized` (7 tests)
5. `TestCostOfPassParametrized` (7 tests)
6. `TestHolmBonferroniParametrized` (7 tests)
7. `TestImplRateParametrized` (7 tests)

**Commit**: `f0dc8d4`
**PR**: #308
**Status**: Merged

### Phase 3: P2-2 Export Tests Expansion

**Files Modified**:
- `tests/unit/analysis/test_export_data.py` (+129 lines)

**New Tests**:
1. `test_json_nan_handler` - NaN/inf conversion
2. `test_compute_statistical_results_empty_df` - Empty DataFrame
3. `test_compute_statistical_results_single_tier` - Single tier edge case
4. `test_compute_statistical_results_degenerate_data` - Small samples
5. `test_compute_statistical_results_correlation_correction` - Holm-Bonferroni
6. `test_export_data_validation_warnings` - NaN validation

**Commit**: `ec93284`
**PR**: #309
**Status**: Merged

## Error Log

### Error 1: Bootstrap CI Test Failures (RESOLVED)

```
FAILED tests/unit/analysis/test_stats_degenerate.py::TestBootstrapCIDegenerate::test_bootstrap_all_same
AssertionError: assert nan == 0.7
```

**Root Cause**: `scipy.stats.bootstrap()` BCa method returns `(mean, NaN, NaN)` for zero-variance data.

**Fix**: Added guard in `stats.py`:

```python
if np.std(data_array) == 0:
    logger.debug("Bootstrap CI called with zero variance data. Returning point estimate as CI bounds.")
    val = float(mean)
    return val, val, val
```

### Error 2: Holm-Bonferroni Test Expectations (RESOLVED)

```
FAILED tests/unit/analysis/test_stats_parametrized.py::TestHolmBonferroniParametrized::test_holm_bonferroni_rejections[mixed_rejections]
E   AssertionError: assert [True, True, False, False] == [True, True, True, False]
```

**Root Cause**: Misunderstood Holm-Bonferroni step-down procedure.

**Fix**: Updated test expectations to match actual behavior:

```python
# Corrected expectations based on step-down procedure
([0.001, 0.01, 0.03, 0.04], [True, True, False, False]),  # Stops at third
([0.001, 0.02, 0.03, 0.04], [True, False, False, False]),  # Stops at second
```

### Error 3: Mann-Whitney Small Sample (RESOLVED)

```
FAILED tests/unit/analysis/test_stats_parametrized.py::TestMannWhitneyUParametrized::test_mann_whitney_significance[clearly_different]
E   assert 0.1 < 0.05
```

**Root Cause**: N=3 samples have too few permutations for reliable p-values.

**Fix**: Increased sample sizes to N=5:

```python
# Updated from N=3 to N=5 for stable p-values
([1, 2, 3, 4, 5], [10, 11, 12, 13, 14], True),  # Now p < 0.05
```

### Error 4: TypeError in Real Data (RESOLVED)

```
TypeError: unsupported operand type(s) for +: 'float' and 'str'
```

**Root Cause**: Real experiment data had string values like `"5.0"` in `criterion.achieved` fields.

**Fix**: Added `safe_float()` helper in `dataframes.py`:

```python
def safe_float(value, default=0.0):
    """Convert value to float, returning default for invalid inputs."""
    try:
        return float(value)
    except (ValueError, TypeError):
        return default
```

## Commands Run

```bash
# Test parametrized tests
pixi run -e analysis pytest tests/unit/analysis/test_stats_parametrized.py -v

# Test export tests
pixi run -e analysis pytest tests/unit/analysis/test_export_data.py -v

# Full analysis suite
pixi run -e analysis pytest tests/unit/analysis/ -v

# Create PRs
gh pr create --title "..." --body-file /tmp/pr_body.txt --label enhancement
gh pr merge --auto --rebase

# Debug Holm-Bonferroni
pixi run -e analysis python3 -c "
from scylla.analysis.stats import holm_bonferroni_correction
p_values = [0.001, 0.01, 0.03, 0.04]
corrected = holm_bonferroni_correction(p_values)
print('Corrected:', corrected)
"
```

## Key Code Snippets

### Defensive Type Coercion

```python
# src/scylla/analysis/dataframes.py:37-46
def safe_float(value, default=0.0):
    """Convert value to float, returning default for invalid inputs."""
    try:
        return float(value)
    except (ValueError, TypeError):
        return default

total_achieved = sum(safe_float(criterion.achieved) for criterion in judge.criteria.values())
total_max = sum(safe_float(criterion.max_points) for criterion in judge.criteria.values())
```

### Zero-Variance Guard

```python
# src/scylla/analysis/stats.py:218-228
def bootstrap_ci(data, confidence=None, n_resamples=None):
    data_array = np.array(data)
    mean = np.mean(data_array)

    # Guard against zero variance
    if np.std(data_array) == 0:
        logger.debug("Bootstrap CI called with zero variance data. Returning point estimate as CI bounds.")
        val = float(mean)
        return val, val, val

    # Normal BCa bootstrap...
```

### NaN for Invalid Operations

```python
# src/scylla/analysis/stats.py:134-135
if n1 == 0 or n2 == 0:
    return np.nan  # Not 0.0!
```

### Centralized Config Access

```python
# src/scylla/analysis/config.py:127-141
@property
def correlation_metrics(self) -> dict[str, str]:
    """Metric pairs for correlation analysis."""
    return self.get("figures", "correlation_metrics", default={
        "score": "Score",
        "cost_usd": "Cost (USD)",
        "total_tokens": "Total Tokens",
        "duration_seconds": "Duration (s)",
    })
```

## Test Results

### Before P2 Improvements
- Total tests: 240
- Stats tests: 36
- Export tests: 2

### After P2 Improvements
- Total tests: 309 (+69, +28%)
- Stats tests: 99 (+63 parametrized)
- Export tests: 8 (+6 edge cases)
- Integration tests: 7 (previously flaky, now stable)

### Test Execution Times
- Parametrized tests: 0.61s (63 tests)
- Export tests: 0.92s (8 tests)
- Full suite: 3.93s (309 tests)

## P2 Items Remaining

Not implemented in this session (future work):

- **P2-4**: Add multi-experiment conflict tests
- **P2-5**: Embed provenance metadata in generated outputs
- **P2-6**: Add --dry-run mode to master orchestrator
- **P2-7**: Validate colorblind accessibility with WCAG contrast ratios

## Lessons Learned

1. **Always verify statistical behavior first** - Don't assume test expectations, run the function
2. **Small samples unreliable** - N≥5 for Mann-Whitney, N≥3 for Shapiro-Wilk
3. **Test degenerate inputs** - Empty, single element, zero variance, all same
4. **NaN vs 0.0** - NaN distinguishes "no data" from "zero effect" in aggregations
5. **Separate PRs** - One improvement per PR makes review easier
6. **Run tests in suite** - Catch test order dependencies early
7. **Defensive programming** - Never modify source data, make analysis code robust

## References

- Publication readiness assessment: Plan file at `~/.claude/plans/imperative-meandering-zebra.md`
- Romano et al. (2006): Cliff's delta thresholds (0.11, 0.28, 0.43)
- BCa bootstrap: Bias-corrected and accelerated method (scipy.stats.bootstrap)
- Holm-Bonferroni: Step-down multiple comparison correction
